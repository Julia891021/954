{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Julia891021/954/blob/0620/0607_5000_oldattention_%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_zFZ6Q-i4rO"
      },
      "source": [
        "訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI-7fQgMWMG7"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH2WBr0wWMvG",
        "outputId": "d81f0559-027b-48ac-8632-66d6a02d6ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWANLW5rt1qB",
        "outputId": "6b8af80e-76f6-49a4-eb55-16f36752d19a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (0.42.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 4.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=882365c0773d66038044b530df429e4d665ea0374270e38920e80226afc5301b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 557 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.6)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=156420 sha256=0f7b40830a88fcf8eee0653d8ed220cc3d0ce2e31622b88907aa55eb3ad274f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "#文字前處理 jieba+去除標點符號\n",
        "import string\n",
        "!pip install jieba\n",
        "import jieba\n",
        "os.chdir('/content/gdrive/Shareddrives/954/文字/jieba')\n",
        "jieba.set_dictionary('dict.txt')\n",
        "import jieba.analyse\n",
        "!pip install emoji\n",
        "import emoji\n",
        "#文字轉向量\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import models\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "!pip install word2vec\n",
        "import word2vec\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "model = gensim.models.Word2Vec.load('/content/gdrive/Shareddrives/954/文字/word2vec_5000.model')\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as tnf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gmq83BB9niam"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,dropout=0.5):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(       \n",
        "            nn.Conv2d(\n",
        "                in_channels=3,\n",
        "                out_channels=16,       \n",
        "                kernel_size=5,        \n",
        "                stride=1,                  \n",
        "                padding=2,                \n",
        "            ),                          \n",
        "            nn.ReLU(),                     \n",
        "            nn.MaxPool2d(kernel_size=5),   \n",
        "        )\n",
        "        self.conv2 = nn.Sequential(       \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),  \n",
        "            nn.ReLU(),                    \n",
        "            nn.MaxPool2d(5),                \n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(1200,500),  # 輸入層與第一隱層結點數設定，全連線結構\n",
        "            torch.nn.Sigmoid(),  # 第一隱層啟用函式採用sigmoid\n",
        "            nn.Linear(500,100),  # 第一隱層與第二隱層結點數設定，全連線結構\n",
        "            torch.nn.Sigmoid(),  # 第一隱層啟用函式採用sigmoid\n",
        "            nn.Linear(100,50),  # 第二隱層與輸出層層結點數設定，全連線結構\n",
        "            torch.nn.Sigmoid(), \n",
        "            nn.Linear(50,2),  \n",
        "            nn.Softmax(dim=1) # 由於有兩個概率輸出，因此對其使用Softmax進行概率歸一化\n",
        "        )\n",
        "        self.dense = nn.Linear(12800,300)\n",
        "        self.attention = nn.Linear(600,2)\n",
        "        #model = gensim.models.Word2Vec.load('/content/gdrive/Shareddrives/954/文字/word2vec.model')\n",
        "        weights = torch.FloatTensor(model.wv.vectors)\n",
        "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
        "        self.embedding.requires_grad = False\n",
        "        self.conv1_text = nn.Sequential(         # input shape (網紅個數,100,100)\n",
        "            nn.Conv1d(\n",
        "                in_channels=100,            # input height\n",
        "                out_channels=32,            # n_filters\n",
        "                kernel_size=7,              # filter size \n",
        "                stride=1,                   # filter movement/step \n",
        "                padding=3,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
        "            ),                              # output shape (網紅個數, 32, 100)\n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool1d(kernel_size=2),    # choose max value in 2, output shape (網紅個數,32, 50) , 100/2=50\n",
        "        )\n",
        "        self.flat = nn.Flatten()  #(網紅個數,32*50)\n",
        "        self.dropout = nn.Dropout(p=dropout)  #(網紅個數,16*50)\n",
        "        self.out_300 = nn.Linear(1600,300) #(16*50,300)\n",
        "    def forward(self):\n",
        "      #--------------------------------文字處理--------------------------------------------------------------\n",
        "      #匯入官方網紅資料\n",
        "      dataPath='/content/gdrive/Shareddrives/954/品牌資料'\n",
        "      def loadExcel_official(dataPath):\n",
        "        i=0\n",
        "        foldername = os.listdir(dataPath)\n",
        "        file_list = []\n",
        "        #print(foldername)\n",
        "        for folder in foldername:\n",
        "          for filename in os.listdir(os.path.join(dataPath,folder)):\n",
        "            if filename=='文字':\n",
        "              for file in os.listdir(os.path.join(dataPath,folder,filename)):\n",
        "                file_list.append(file)\n",
        "                if i==0:\n",
        "                  df=pd.read_excel(os.path.join(dataPath,folder,filename,file))\n",
        "                  df_new=df\n",
        "                  i+=1\n",
        "                else:\n",
        "                  df=pd.read_excel(os.path.join(dataPath,folder,filename,file))\n",
        "                  df_new=pd.concat([df_new,df],axis=1,ignore_index=True)\n",
        "                  i+=1\n",
        "        return df_new, file_list, folder\n",
        "\n",
        "      df_official, official_name, folder =loadExcel_official(dataPath)\n",
        "      df_official = pd.concat([df_official[0], df_official[1], df_official[2]], ignore_index = True)\n",
        "      df_official = pd.DataFrame(df_official)\n",
        "\n",
        "      #匯入網紅資料\n",
        "      dataPath='/content/gdrive/Shareddrives/954/網紅資料' \n",
        "      def loadExcel(dataPath, brand):\n",
        "        df = pd.read_excel('/content/gdrive/Shareddrives/954/網紅資料/網紅_分類表.xlsx')\n",
        "        celebrity = pd.concat([df['celebrity(folder_name)'], df['mouggan']], axis = 1)\n",
        "        celebrity.set_index('celebrity(folder_name)',inplace=True)\n",
        "        official = []\n",
        "        for name in official_name:\n",
        "          name = name.strip('.xlsx')\n",
        "          official.append(name)\n",
        "        celebrity = celebrity.drop(index = official)\n",
        "        foldername = celebrity.index\n",
        "        i=0\n",
        "        j=0\n",
        "        for folder in foldername:\n",
        "          if celebrity[brand][folder]==1:\n",
        "            for filename in os.listdir(os.path.join(dataPath,folder)):\n",
        "              if filename=='文字':\n",
        "                for file in os.listdir(os.path.join(dataPath,folder,filename)):\n",
        "                  if i==0:\n",
        "                    df_one=pd.read_excel(os.path.join(dataPath,folder,filename,file))\n",
        "                    df_new_one=df_one\n",
        "                    i+=1\n",
        "                  else:\n",
        "                    df_one=pd.read_excel(os.path.join(dataPath,folder,filename,file))\n",
        "                    df_new_one=pd.concat([df_new_one,df_one],axis=1,ignore_index=True)\n",
        "                    i+=1\n",
        "          if celebrity[brand][folder]==0:\n",
        "            for filename in os.listdir(os.path.join(dataPath,folder)):\n",
        "              if filename=='文字':\n",
        "                for file in os.listdir(os.path.join(dataPath,folder,filename)):\n",
        "                  if j==0:\n",
        "                    df_zero=pd.read_excel(os.path.join(dataPath,folder,filename,file))\n",
        "                    df_new_zero=df_zero\n",
        "                    j+=1\n",
        "                  else:\n",
        "                    df_zero=pd.read_excel(os.path.join(dataPath,folder,filename,file))\n",
        "                    df_new_zero=pd.concat([df_new_zero,df_zero],axis=1,ignore_index=True)\n",
        "                    j+=1\n",
        "        return df_new_one,df_new_zero\n",
        "      df_one,df_zero=loadExcel(dataPath,'mouggan')\n",
        "\n",
        "      #文字前處理\n",
        "      def text_preprocessing(df):\n",
        "        #所有貼文合併成一篇\n",
        "        def text_concate(df):\n",
        "          result = []\n",
        "          for j in range(df.shape[1]): \n",
        "              new1 = []\n",
        "              for i in range(df.shape[0]):\n",
        "                if df[j][i] != float('nan'):\n",
        "                  new1.append(df[j][i])\n",
        "              new2 = \"\".join(str(new1))\n",
        "              result.append(new2)\n",
        "          return result\n",
        "        df_new = pd.DataFrame(text_concate(df), columns = ['text'])\n",
        "\n",
        "        #emoji to text\n",
        "        def emoji_to_text(df):\n",
        "          #print(df['text'])\n",
        "          for i in range(df_new.shape[0]):\n",
        "            df['text'][i] = emoji.demojize(df['text'][i], delimiters=(\"\",\"\"))\n",
        "          return df\n",
        "        df_new = emoji_to_text(df_new)\n",
        "        def remove_punctuation(df_new):\n",
        "          for i in range(df_new.shape[0]):\n",
        "            df_new['text'][i] = ''.join([i for i in df_new['text'][i] if i not in string.punctuation])\n",
        "          return df_new\n",
        "        df_new = remove_punctuation(df_new)\n",
        "        #jieba斷句\n",
        "        def jieba_text(df):\n",
        "          for i in range(df.shape[0]):\n",
        "            word = []\n",
        "            word.append(list(jieba.cut(df['text'][i], cut_all=False))[:-1]) # 精確模式\n",
        "            for j in range(len(word)):\n",
        "              sentence = ' '.join(word[0])\n",
        "            df['text'][i] = sentence\n",
        "            #print(word[0])\n",
        "          return df\n",
        "        df_new = jieba_text(df_new)\n",
        "        return df_new\n",
        "      df_brand_zero_cut = text_preprocessing(df_zero)\n",
        "      df_brand_one_cut = text_preprocessing(df_one)\n",
        "      df_brand_official_cut= text_preprocessing(df_official)\n",
        "      df_brand_zero_cut = df_brand_zero_cut.drop([14])\n",
        "      df_brand_zero_cut = df_brand_zero_cut.reset_index()\n",
        "\n",
        "      #word2vec轉向量\n",
        "      #load word2vec pre-trained model\n",
        "      def word_to_vector(df):\n",
        "        def word_split(tokens, size):\n",
        "            vec = np.zeros(size).reshape((1, size))\n",
        "            count = 0\n",
        "            for word in tokens:\n",
        "                try:\n",
        "                    vec += model[word].reshape((1, size))\n",
        "                    count += 1.\n",
        "                except KeyError:  # handling the case where the token is not in vocabulary\n",
        "                    continue\n",
        "            if count != 0:\n",
        "                vec /= count\n",
        "            return vec\n",
        "        #轉成array的形式等等丟model\n",
        "        tokenized_tweet = df['text'].apply(lambda x: x.split())\n",
        "        # tokenized_tweet = tokenized_tweet.tolist()\n",
        "        #wordvec_arrays = np.zeros((len(tokenized_tweet), 100))\n",
        "        query_id = torch.tensor(np.zeros((len(tokenized_tweet),100)),dtype=torch.int64)\n",
        "        for i in range(len(tokenized_tweet)):\n",
        "          for j in range(100):\n",
        "            if tokenized_tweet[i][j] in model.wv:\n",
        "              query_id[i][j] = torch.tensor(model.wv.vocab[tokenized_tweet[i][j]].index)\n",
        "            else:\n",
        "              continue\n",
        "        #w2v_df = torch.tensor(wordvec_arrays)\n",
        "        return query_id\n",
        "\n",
        "      official_input = word_to_vector(df_brand_official_cut)\n",
        "      one_input = word_to_vector(df_brand_one_cut)\n",
        "      zero_input = word_to_vector(df_brand_zero_cut)\n",
        "\n",
        "      def cnn_text(x):       #torch.Size([網紅個數, 100])\n",
        "        #print(x.shape)\n",
        "        x = self.embedding(x)  #torch.Size([網紅個數, 100, 100])\n",
        "        #print(x.shape)\n",
        "        x = self.dropout(x)   #torch.Size([網紅個數, 100, 100])\n",
        "        #print(x.shape)\n",
        "        x = self.conv1_text(x)  #torch.Size([網紅個數, 32, 50])\n",
        "        #print(x.shape)\n",
        "        x = self.dropout(x)   #torch.Size([網紅個數, 32, 50])\n",
        "        #print(x.shape)\n",
        "        x = self.flat(x)    #torch.Size([網紅個數, 1600])\n",
        "        #print(x.shape)\n",
        "        x = self.dropout(x)  #torch.Size([網紅個數, 1600])\n",
        "        #print(x.shape)\n",
        "        x = self.out_300(x)   #torch.Size([網紅個數, 300])\n",
        "        #print(x.shape)\n",
        "        return x    # return x for visualization\n",
        "      \n",
        "      output_official_cnn = cnn_text(official_input)\n",
        "      output_one_cnn = cnn_text(one_input)\n",
        "      output_zero_cnn = cnn_text(zero_input)\n",
        "\n",
        "      def concat_brand(tensor_other,tensor_brand):\n",
        "        for i in range(tensor_other.shape[0]):\n",
        "          if i==0:\n",
        "            tensor_brand_new=tensor_brand\n",
        "            i+=1\n",
        "          else:\n",
        "            tensor_brand_new=torch.cat((tensor_brand_new,tensor_brand), 0)\n",
        "            i+=1\n",
        "        output = torch.cat((tensor_other, tensor_brand_new), 1)\n",
        "        return output\n",
        "      #--------------------------------圖片處理--------------------------------------------------------------\n",
        "      # 品牌\n",
        "      os.chdir('/content/gdrive/Shareddrives/954/品牌資料/' + 'mouggan/'+'圖片')\n",
        "      img_list = os.listdir()\n",
        "      result = []\n",
        "      for i in range (60) :\n",
        "              if (img_list[i] != '.DS_Store'):\n",
        "                  img = cv2.imread( img_list[i] )\n",
        "                  img = cv2.resize( img, (500, 500))\n",
        "                  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                  transform = transforms.Compose([\n",
        "                      transforms.ToTensor()\n",
        "                  ])\n",
        "                  tensor = transform(img)\n",
        "                  tensor = torch.reshape( tensor, ( 1,3,500,500) )\n",
        "                  tensor = self.conv1(tensor)\n",
        "                  tensor = self.conv2(tensor)\n",
        "                  output = tensor.view(tensor.size(0), -1) \n",
        "                  output = self.dense(output)# !change(1,12800) to (1,300)\n",
        "                  output = output.detach().numpy()\n",
        "                  result.append(output)\n",
        "      result= torch.tensor(result).float()\n",
        "      result = torch.reshape(result, (result.shape[0], result.shape[2]))\n",
        "      result = result.detach().numpy()\n",
        "      mean = np.mean(result, axis=0) \n",
        "      result = []\n",
        "      result.append(mean)\n",
        "      brand_img_vector= torch.tensor(result).float()\n",
        "      \n",
        "      # 網紅\n",
        "      os.chdir('/content/gdrive/Shareddrives/954/網紅資料/')\n",
        "      excel = pd.read_excel('網紅_分類表.xlsx')\n",
        "      folder = excel['celebrity(folder_name)']\n",
        "      brand = excel['mouggan']\n",
        "\n",
        "      folder_1 = []\n",
        "      folder_0 = []\n",
        "      for i in range(len(brand)):\n",
        "        if brand[i] == 1:\n",
        "          folder_1.append(i)\n",
        "        else:\n",
        "          folder_0.append(i)\n",
        "\n",
        "      celebrity_img_0 = []\n",
        "      celebrity_img_1 = []\n",
        "      for i in range(len(folder_1)):\n",
        "        celebrity_img_1.append(folder[folder_1[i]])\n",
        "      for i in range(len(folder_0)):\n",
        "        celebrity_img_0.append(folder[folder_0[i]])\n",
        "      celebrity_img_1.remove('mouggan')\n",
        "      celebrity_img_1.remove('manjumilin')\n",
        "      celebrity_img_1.remove('egg204')\n",
        "\n",
        "      celebrity_img_vector_1 = torch.tensor([])\n",
        "      for folder_name in celebrity_img_1 :\n",
        "        os.chdir('/content/gdrive/Shareddrives/954/網紅資料/' + folder_name + '/圖片' )\n",
        "        img_list = os.listdir()\n",
        "        result = []\n",
        "        for i in range (20) :\n",
        "            if (img_list[i] != '.DS_Store'):\n",
        "                img = cv2.imread( img_list[i] )\n",
        "                img = cv2.resize( img, (500, 500))\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                transform = transforms.Compose([\n",
        "                    transforms.ToTensor()\n",
        "                ])\n",
        "                tensor = transform(img)\n",
        "                tensor = torch.reshape( tensor, ( 1,3,500,500) )\n",
        "                tensor = self.conv1(tensor)\n",
        "                tensor = self.conv2(tensor)\n",
        "                output = tensor.view(tensor.size(0), -1) \n",
        "                output = self.dense(output)# !change(1,12800) to (1,300)\n",
        "                output = output.detach().numpy()\n",
        "                result.append(output)\n",
        "        result= torch.tensor(result).float()\n",
        "        result = torch.reshape( result, (result.shape[0], result.shape[2]))\n",
        "        result = result.detach().numpy()\n",
        "        mean = np.mean(result, axis=0) \n",
        "        result = []\n",
        "        result.append(mean)\n",
        "        result= torch.tensor(result).float()\n",
        "        celebrity_img_vector_1 = torch.cat( ( celebrity_img_vector_1, result), 0)\n",
        "      celebrity_1 = torch.cat((celebrity_img_vector_1, output_one_cnn), 1)  \n",
        "      celebrity_1 = celebrity_1.repeat(2,1) \n",
        "\n",
        "      celebrity_img_vector_0 = torch.tensor([])\n",
        "      for folder_name in celebrity_img_0 :\n",
        "        os.chdir('/content/gdrive/Shareddrives/954/網紅資料/' + folder_name + '/圖片' )\n",
        "        img_list = os.listdir()\n",
        "        result = []\n",
        "        for i in range (20) :\n",
        "          if (img_list[i] != '.DS_Store'):\n",
        "              img = cv2.imread( img_list[i] )\n",
        "              img = cv2.resize( img, (500, 500))\n",
        "              img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "              transform = transforms.Compose([\n",
        "                  transforms.ToTensor()\n",
        "              ])\n",
        "              tensor = transform(img)\n",
        "              tensor = torch.reshape( tensor, ( 1,3,500,500) )\n",
        "              tensor = self.conv1(tensor)\n",
        "              tensor = self.conv2(tensor)\n",
        "              output = tensor.view(tensor.size(0), -1) \n",
        "              output = self.dense(output)# !change(1,12800) to (1,300)\n",
        "              output = output.detach().numpy()\n",
        "              result.append(output)\n",
        "        result= torch.tensor(result).float()\n",
        "        result = torch.reshape( result, (result.shape[0], result.shape[2]))\n",
        "        result = result.detach().numpy()\n",
        "        mean = np.mean(result, axis=0) \n",
        "        result = []\n",
        "        result.append(mean)\n",
        "        result= torch.tensor(result).float()\n",
        "        celebrity_img_vector_0 = torch.cat( ( celebrity_img_vector_0, result), 0)\n",
        "      celebrity_0 = torch.cat((celebrity_img_vector_0, output_zero_cnn), 1)\n",
        "\n",
        "      brand_img_vector = brand_img_vector.repeat(366,1)\n",
        "      output_official_cnn = output_official_cnn.repeat(366,1)\n",
        "      brand_all = torch.cat((brand_img_vector, output_official_cnn), 1) \n",
        "      celebrity_all = torch.cat((celebrity_0, celebrity_1), 0) \n",
        "\n",
        "      # attention 標準注意力\n",
        "      for i in range(celebrity_all.shape[0]):\n",
        "        image = celebrity_all[i][:300].reshape(1,300)\n",
        "        text = celebrity_all[i][300:].reshape(1,300)\n",
        "        all = torch.cat((image,text),0)\n",
        "        all = all.detach().numpy()\n",
        "        all_weight = np.matmul(all,all.T)\n",
        "        all_weight = torch.tensor(all_weight).float()\n",
        "        all_weight=tnf.softmax(all_weight,dim=1)\n",
        "        all_weight=all_weight.detach().numpy()\n",
        "        all_new=np.matmul(all_weight,all)\n",
        "        all_new = torch.tensor(all_new).float()\n",
        "        celebrity_all[i] = all_new.reshape(600)\n",
        "\n",
        "      for i in range(brand_all.shape[0]):\n",
        "        image = brand_all[i][:300].reshape(1,300)\n",
        "        text = brand_all[i][300:].reshape(1,300)\n",
        "        all = torch.cat((image,text),0)\n",
        "        all = all.detach().numpy()\n",
        "        all_weight = np.matmul(all,all.T)\n",
        "        all_weight = torch.tensor(all_weight).float()\n",
        "        all_weight=tnf.softmax(all_weight,dim=1)\n",
        "        all_weight=all_weight.detach().numpy()\n",
        "        all_new=np.matmul(all_weight,all)\n",
        "        all_new = torch.tensor(all_new).float()\n",
        "        brand_all[i] = all_new.reshape(600)\n",
        "\n",
        "      # # attention 神經網路\n",
        "      # for i in range(celebrity_all.shape[0]):\n",
        "      #   image = celebrity_all[i][:300].reshape(1,300)\n",
        "      #   text = celebrity_all[i][300:].reshape(1,300)\n",
        "      #   all = celebrity_all[i]\n",
        "      #   all_weight = self.attention(all).reshape(1,2)\n",
        "      #   all_weight = tnf.softmax(all_weight,dim=1)\n",
        "      #   text_weight = all_weight[0][0].reshape(1,1).float()\n",
        "      #   image_weight = all_weight[0][1].reshape(1,1).float()\n",
        "      #   weighted_text = torch.mm(text_weight,text)\n",
        "      #   weighted_image = torch.mm(image_weight,image)\n",
        "      #   all_new = torch.cat((weighted_text,weighted_image),1)\n",
        "      #   celebrity_all[i] = all_new.reshape(600)\n",
        "\n",
        "      # for i in range(brand_all.shape[0]):\n",
        "      #   image = brand_all[i][:300].reshape(1,300)\n",
        "      #   text = brand_all[i][300:].reshape(1,300)\n",
        "      #   all = brand_all[i]\n",
        "      #   all_weight = self.attention(all).reshape(1,2)\n",
        "      #   all_weight = tnf.softmax(all_weight,dim=1)\n",
        "      #   text_weight = all_weight[0][0].reshape(1,1).float()\n",
        "      #   image_weight = all_weight[0][1].reshape(1,1).float()\n",
        "      #   weighted_text = torch.mm(text_weight,text)\n",
        "      #   weighted_image = torch.mm(image_weight,image)\n",
        "      #   all_new = torch.cat((weighted_text,weighted_image),1)\n",
        "      #   brand_all[i] = all_new.reshape(600)\n",
        "\n",
        "      print(celebrity_all.shape)\n",
        "      print(brand_all.shape)\n",
        "      x_t = torch.cat((celebrity_all, brand_all), 1) \n",
        "      output = self.out(x_t)\n",
        "\n",
        "      return output  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBoCbgl_d1Gg",
        "outputId": "30547827-87d6-43a6-a60d-3d506aec20b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (out): Sequential(\n",
              "    (0): Linear(in_features=1200, out_features=500, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): Linear(in_features=500, out_features=100, bias=True)\n",
              "    (3): Sigmoid()\n",
              "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (5): Sigmoid()\n",
              "    (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    (7): Softmax(dim=1)\n",
              "  )\n",
              "  (dense): Linear(in_features=12800, out_features=300, bias=True)\n",
              "  (attention): Linear(in_features=600, out_features=2, bias=True)\n",
              "  (embedding): Embedding(4237114, 100)\n",
              "  (conv1_text): Sequential(\n",
              "    (0): Conv1d(100, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (out_300): Linear(in_features=1600, out_features=300, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "cnn = CNN()\n",
        "cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzFaRvLbjDaY",
        "outputId": "f38568c9-c28b-4a0e-a05d-b8fa7ee453fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from /content/gdrive/Shareddrives/954/文字/jieba/dict.txt ...\n",
            "Dumping model to file cache /tmp/jieba.u991154c518f2a731fe75d5ff0d98fecd.cache\n",
            "Loading model cost 0.773 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:251: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6925977468490601\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6925320029258728\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.692625105381012\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6924645304679871\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6924841403961182\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6924542188644409\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6924432516098022\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6924879550933838\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6924780607223511\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6924229264259338\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6923584938049316\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6923707723617554\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6923573017120361\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6923081278800964\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6923253536224365\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6923345327377319\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922482252120972\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6923344731330872\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922733783721924\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922247409820557\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922614574432373\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922422051429749\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921765208244324\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922071576118469\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921027898788452\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922139525413513\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.692283570766449\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922231912612915\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921597719192505\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921891570091248\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921027898788452\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922081112861633\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6922149658203125\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920689940452576\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921457052230835\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920396089553833\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921455264091492\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921457648277283\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921464800834656\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921116709709167\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920257210731506\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920181512832642\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.692038893699646\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920799612998962\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920127868652344\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920467615127563\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920798420906067\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.692025899887085\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920466423034668\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920425891876221\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920859813690186\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6921177506446838\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.691991925239563\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920431852340698\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6919829845428467\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920225620269775\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6919939517974854\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.692048192024231\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920396089553833\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920287013053894\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6920350193977356\n",
            "torch.Size([366, 600])\n",
            "torch.Size([366, 600])\n",
            "0.6919881105422974\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.SGD(cnn.parameters(),lr=0.01) # 優化器使用隨機梯度下降，傳入網路引數和學習率\n",
        "loss_func = torch.nn.CrossEntropyLoss() # 損失函式使用交叉熵損失函式\n",
        "\n",
        "y_1_t = torch.ones(174) # Y 9+78=87 87*2=174\n",
        "y_0_t = torch.zeros(192) # Y\n",
        "\n",
        "y_t = torch.cat((y_0_t,y_1_t),0)\n",
        "# 模型訓練\n",
        "num_epoch = 150 # 最大迭代更新次數\n",
        "for epoch in range(num_epoch):\n",
        "  y_p = cnn()  # 喂資料並前向傳播\n",
        "  loss = loss_func(y_p,y_t.long()) # 計算損失\n",
        "  optimizer.zero_grad()  # 清除梯度\n",
        "  loss.backward()  # 計算梯度，誤差回傳\n",
        "  optimizer.step()  # 根據計算的梯度，更新網路中的引數\n",
        "    # if epoch % 10 == 0:\n",
        "    #     print('epoch: {}, loss: {}'.format(epoch, loss.data.item()))\n",
        "  print(loss.data.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-PearVWgX5w"
      },
      "outputs": [],
      "source": [
        "print(\"所有樣本的預測標籤: \\n\",torch.max(y_p,dim = 1)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZYpOuOh8A8-"
      },
      "source": [
        "attention 測試"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N_tCVXJNH-1"
      },
      "outputs": [],
      "source": [
        "# 神經網路注意力\n",
        "celebrity_all = torch.rand(201,600)\n",
        "brand_all = torch.rand(201,600)\n",
        "\n",
        "# attention 神經網路\n",
        "for i in range(celebrity_all.shape[0]):\n",
        "  image = celebrity_all[i][:300].reshape(1,300)\n",
        "  text = celebrity_all[i][300:].reshape(1,300)\n",
        "  all = celebrity_all[i]\n",
        "  all_weight = self.attention(all)\n",
        "  all_weight = tnf.softmax(all_weight,dim=1)\n",
        "  text_weight = all_weight[0][0].reshape(1,1).float()\n",
        "  image_weight = all_weight[0][1].reshape(1,1).float()\n",
        "  weighted_text = torch.mm(text_weight,text)\n",
        "  weighted_image = torch.mm(image_weight,image)\n",
        "  all_new = torch.cat((weighted_text,weighted_image),1)\n",
        "  celebrity_all[i] = all_new.reshape(600)\n",
        "\n",
        "for i in range(brand_all.shape[0]):\n",
        "  image = brand_all[i][:300].reshape(1,300)\n",
        "  text = brand_all[i][300:].reshape(1,300)\n",
        "  all = brand_all[i]\n",
        "  all_weight = self.attention(all)\n",
        "  all_weight = tnf.softmax(all_weight,dim=1)\n",
        "  text_weight = all_weight[0][0].reshape(1,1).float()\n",
        "  image_weight = all_weight[0][1].reshape(1,1).float()\n",
        "  weighted_text = torch.mm(text_weight,text)\n",
        "  weighted_image = torch.mm(image_weight,image)\n",
        "  all_new = torch.cat((weighted_text,weighted_image),1)\n",
        "  brand_all[i] = all_new.reshape(600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR7ftviOcJqy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "0607_5000_oldattention 的副本",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}